{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpermodules.memory_handling import PickleHelper\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = PickleHelper.pickle_load(filename).obj\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# applying data cleaning to new csv file\n",
    "merged_df.to_csv('olist_merged_data.csv', index=False)\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for missing values by percentage in each column\n",
    "merged_df.isnull().sum() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# check for missing values by percentage in each column\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean and preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with more than 50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \n",
    "                    'order_delivered_customer_date', 'order_estimated_delivery_date', \n",
    "                    'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate new features\n",
    "    df['time_to_delivery'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.days\n",
    "    df['order_processing_time'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.days\n",
    "    df['estimated_vs_actual_shipping'] = (df['order_estimated_delivery_date'] - df['order_delivered_customer_date']).dt.days\n",
    "    df['product_volume_m3'] = (df['product_length_cm'] * df['product_width_cm'] * df['product_height_cm']) / 1000000\n",
    "    df['satisfaction'] = (df['review_score'] >= 4).astype(int)\n",
    "    df['order_value'] = df['price'] + df['freight_value']\n",
    "\n",
    "    # create late delivery flag\n",
    "    df['late_delivery'] = (df['order_delivered_customer_date'] > df['order_estimated_delivery_date']).astype(int)\n",
    "\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # create seasonal features from order_purchase_timestamp\n",
    "    df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['order_day'] = df['order_purchase_timestamp'].dt.dayofweek\n",
    "    df['order_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = preprocess_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop unnecessary columns\n",
    "merged_df.drop(['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'review_score', 'seller_zip_code_prefix']\n",
    "               , axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataset\n",
    "merged_df.to_csv('olist_merged_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the CSAT percentage\n",
    "merged_df['satisfaction'].value_counts() / len(merged_df) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
