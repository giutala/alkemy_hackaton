{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpermodules.memory_handling import PickleHelper\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = PickleHelper.pickle_load(filename).obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying data cleaning to new csv file\n",
    "merged_df.to_csv('olist_merged_data.csv', index=False)\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for missing values by percentage in each column\n",
    "merged_df.isnull().sum() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop missing values column with more than 50% missing values\n",
    "merged_df = merged_df.dropna(thresh=len(merged_df) * 0.5, axis=1)\n",
    "\n",
    "# drop rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# check for missing values by percentage in each column\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean and preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with more than 50% missing values\n",
    "    df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \n",
    "                    'order_delivered_customer_date', 'order_estimated_delivery_date', \n",
    "                    'shipping_limit_date', 'review_creation_date', 'review_answer_timestamp']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate new features\n",
    "    df['time_to_delivery'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.days\n",
    "    df['order_processing_time'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.days\n",
    "    df['estimated_vs_actual_shipping'] = (df['order_estimated_delivery_date'] - df['order_delivered_customer_date']).dt.days\n",
    "    df['product_volume_m3'] = (df['product_length_cm'] * df['product_width_cm'] * df['product_height_cm']) / 1000000\n",
    "    df['satisfaction'] = (df['review_score'] >= 4).astype(int)\n",
    "    df['order_value'] = df['price'] + df['freight_value']\n",
    "\n",
    "    # create late delivery flag\n",
    "    df['late_delivery'] = (df['order_delivered_customer_date'] > df['order_estimated_delivery_date']).astype(int)\n",
    "\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # create seasonal features from order_purchase_timestamp\n",
    "    df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['order_day'] = df['order_purchase_timestamp'].dt.dayofweek\n",
    "    df['order_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = preprocess_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop unnecessary columns\n",
    "merged_df.drop(['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'review_score', 'seller_zip_code_prefix']\n",
    "               , axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataset\n",
    "merged_df.to_csv('olist_merged_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the CSAT percentage\n",
    "merged_df['satisfaction'].value_counts() / len(merged_df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation for numerical values (Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "# Plot the heatmap using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Matrix (Pearson)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation for categorical values (Cramer's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cramér's V\n",
    "def cramers_v(x, y):\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    \n",
    "    # Perform Chi-Square test\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Calculate Cramér's V\n",
    "    n = contingency_table.sum().sum()\n",
    "    r, k = contingency_table.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r-1, k-1))))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['list of categorical column']\n",
    "\n",
    "# Create an empty matrix to store Cramér's V values\n",
    "n = len(categorical_columns)\n",
    "cramers_v_matrix = pd.DataFrame(np.zeros((n, n)), index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "# Calculate Cramér's V for each pair of variables\n",
    "for col1 in categorical_columns:\n",
    "    for col2 in categorical_columns:\n",
    "        cramers_v_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5)\n",
    "plt.title(\"Cramér's V Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Data Stardadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(df, columns_to_scale): \n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Apply the Min-Max scaling (normalization) to the dataset\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns_to_scale)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to scale\n",
    "columns_to_scale = ['list of columns']\n",
    "\n",
    "# Scale Data\n",
    "df_scaled = data_scaler(df, columns_to_scale)\n",
    "\n",
    "# Display the normalized dataset\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nNormalized Data:\\n\", df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance test (numerical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VarianceThreshold with a threshold of 0.062\n",
    "selector = VarianceThreshold(threshold=0.062)\n",
    "selected_features = selector.fit_transform(df)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_columns = df.columns[selector.get_support()]\n",
    "\n",
    "# Create a new DataFrame with selected features\n",
    "df_selected = pd.DataFrame(selected_features, columns=selected_columns)\n",
    "\n",
    "# Display the selected features\n",
    "print(\"Selected Features Based on Variance Threshold (0.062):\")\n",
    "print(df_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square test (categorical values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Define the target variable\n",
    "target = 'class'\n",
    "\n",
    "# List to store Chi-Square values\n",
    "chi2_values = []\n",
    "features = []\n",
    "\n",
    "# Iterate over categorical columns (excluding the target)\n",
    "for column in df.columns:\n",
    "    if column != target:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[column], df[target])\n",
    "        \n",
    "        # Perform the Chi-Square test\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "        # Store the Chi-Square value and feature name\n",
    "        chi2_values.append(chi2)\n",
    "        features.append(column)\n",
    "\n",
    "        # Display the results for each categorical variable\n",
    "        print(f\"\\nChi-Square Test Results for '{column}':\")\n",
    "        print(f\"Contingency Table:\\n{contingency_table}\")\n",
    "        print(f\"Chi-Square Statistic: {chi2}\")\n",
    "        print(f\"P-Value: {p}\")\n",
    "        print(f\"Degrees of Freedom: {dof}\")\n",
    "        print(\"Expected Frequencies:\\n\", expected)\n",
    "\n",
    "# Plotting the Chi-Square values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(features, chi2_values, color='skyblue')\n",
    "plt.title('Chi-Square Values for Categorical Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Chi-Square Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05  # Significance level\n",
    "for i, p in enumerate([stats.chi2_contingency(pd.crosstab(df[column], df[target]))[1] for column in features]):\n",
    "    if p < alpha:\n",
    "        print(f\"\\nReject the null hypothesis for '{features[i]}': There is a significant association.\")\n",
    "    else:\n",
    "        print(f\"\\nFail to reject the null hypothesis for '{features[i]}': There is no significant association.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Fit and transform the categorical variables\n",
    "encoded_features = encoder.fit_transform(df[['incident_type', 'collision_type']])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['incident_type', 'collision_type']))\n",
    "\n",
    "# Concatenate the original numerical data with the one-hot encoded DataFrame\n",
    "df_encoded = pd.concat([df.drop(['incident_type', 'collision_type'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display the DataFrame after one-hot encoding\n",
    "print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_score(df, feature, target):\n",
    "    classes = df[target].unique()\n",
    "    mean = []\n",
    "    variance = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        mean.append(df[df[target] == cls][feature].mean())\n",
    "        variance.append(df[df[target] == cls][feature].var(ddof=0))  # Population variance\n",
    "    \n",
    "    # Fisher score calculation\n",
    "    fisher_score = (mean[0] - mean[1])**2 / (variance[0] + variance[1])\n",
    "    return fisher_score\n",
    "\n",
    "# Calculate Fisher scores for each feature\n",
    "fisher_scores = {}\n",
    "for feature in df.columns[:-1]:  # Exclude the target column\n",
    "    fisher_scores[feature] = fisher_score(df, feature, 'class')\n",
    "\n",
    "# Create a DataFrame to display Fisher scores\n",
    "fisher_scores_df = pd.DataFrame(list(fisher_scores.items()), columns=['Feature', 'Fisher Score'])\n",
    "\n",
    "# Display the Fisher scores\n",
    "print(\"Fisher Scores for Customer Features:\")\n",
    "print(fisher_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
